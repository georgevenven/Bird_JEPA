{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 73])\n",
      "torch.Size([73])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "pt_file = \"/media/george-vengrovski/Desk SSD/BirdJEPA/llb3_val/llb3_0001_2018_04_23_14_18_03.pt\"\n",
    "data = torch.load(pt_file)\n",
    "print(data['s'].shape)\n",
    "print(data['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env wired up, starting download…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e3c1241bb74e7589e351924ef54297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POW_test5s_shard_0001.tar.gz:   0%|          | 0.00/199M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eaa6f0625b34b89a8c2cfe88d7085f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "XCM_metadata.parquet:   0%|          | 0.00/16.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339e3bed3bf54e59b53cc50f4e91acdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POW_metadata_test_5s.parquet:   0%|          | 0.00/111k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66bfd1976524caeb98ba729dfe846c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting train split:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c16d6f6a454c4c9cb6b06e1e6f4280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting valid split:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fec91b11ac495ca8b4fb1476c17656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 8 back to 1 for the valid split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b2fd46ddac54f56871921e8caa55437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materializing split: train\n",
      "  Processed 1000 items in train...\n",
      "  Processed 2000 items in train...\n",
      "  Processed 3000 items in train...\n",
      "  Processed 4000 items in train...\n",
      "  Processed 5000 items in train...\n",
      "  Processed 6000 items in train...\n",
      "  Processed 7000 items in train...\n",
      "  Processed 8000 items in train...\n",
      "  Processed 9000 items in train...\n",
      "  Processed 10000 items in train...\n",
      "  Processed 11000 items in train...\n",
      "  Processed 12000 items in train...\n",
      "  Processed 13000 items in train...\n",
      "  Processed 14000 items in train...\n",
      "  Processed 15000 items in train...\n",
      "  Processed 16000 items in train...\n",
      "  Processed 17000 items in train...\n",
      "  Processed 18000 items in train...\n",
      "  Processed 19000 items in train...\n",
      "  Processed 20000 items in train...\n",
      "  Processed 21000 items in train...\n",
      "  Processed 22000 items in train...\n",
      "  Processed 23000 items in train...\n",
      "  Processed 24000 items in train...\n",
      "  Processed 25000 items in train...\n",
      "  Processed 26000 items in train...\n",
      "  Processed 27000 items in train...\n",
      "  Processed 28000 items in train...\n",
      "  Processed 29000 items in train...\n",
      "  Processed 30000 items in train...\n",
      "  Processed 31000 items in train...\n",
      "  Processed 32000 items in train...\n",
      "  Processed 33000 items in train...\n",
      "  Processed 34000 items in train...\n",
      "  Processed 35000 items in train...\n",
      "  Processed 36000 items in train...\n",
      "  Processed 37000 items in train...\n",
      "  Processed 38000 items in train...\n",
      "  Processed 39000 items in train...\n",
      "  Processed 40000 items in train...\n",
      "  Processed 41000 items in train...\n",
      "  Processed 42000 items in train...\n",
      "  Processed 43000 items in train...\n",
      "  Processed 44000 items in train...\n",
      "  Processed 45000 items in train...\n",
      "  Processed 46000 items in train...\n",
      "  Processed 47000 items in train...\n",
      "  Processed 48000 items in train...\n",
      "  Processed 49000 items in train...\n",
      "  Processed 50000 items in train...\n",
      "  Processed 51000 items in train...\n",
      "  Processed 52000 items in train...\n",
      "  Processed 53000 items in train...\n",
      "  Processed 54000 items in train...\n",
      "  Processed 55000 items in train...\n",
      "  Processed 56000 items in train...\n",
      "  Processed 57000 items in train...\n",
      "  Processed 58000 items in train...\n",
      "  Processed 59000 items in train...\n",
      "  Processed 60000 items in train...\n",
      "  Processed 61000 items in train...\n",
      "  Processed 62000 items in train...\n",
      "  Processed 63000 items in train...\n",
      "  Processed 64000 items in train...\n",
      "  Processed 65000 items in train...\n",
      "  Processed 66000 items in train...\n",
      "  Processed 67000 items in train...\n",
      "  Processed 68000 items in train...\n",
      "  Processed 69000 items in train...\n",
      "  Processed 70000 items in train...\n",
      "  Processed 71000 items in train...\n",
      "  Processed 72000 items in train...\n",
      "  Processed 73000 items in train...\n",
      "  Processed 74000 items in train...\n",
      "  Processed 75000 items in train...\n",
      "  Processed 76000 items in train...\n",
      "  Processed 77000 items in train...\n",
      "  Processed 78000 items in train...\n",
      "  Processed 79000 items in train...\n",
      "  Processed 80000 items in train...\n",
      "  Processed 81000 items in train...\n",
      "  Processed 82000 items in train...\n",
      "  Processed 83000 items in train...\n",
      "  Processed 84000 items in train...\n",
      "  Processed 85000 items in train...\n",
      "  Processed 86000 items in train...\n",
      "  Processed 87000 items in train...\n",
      "  Processed 88000 items in train...\n",
      "  Processed 89000 items in train...\n",
      "Materializing split: valid\n",
      "  Processed 1000 items in valid...\n",
      "  Processed 2000 items in valid...\n",
      "  Processed 3000 items in valid...\n",
      "  Processed 4000 items in valid...\n",
      "✓ finished XCM in 18.3 min  (all data lives under “/home/george-vengrovski/Documents/data/BirdSet”)\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "# location with the space intact\n",
    "BIG = \"/home/george-vengrovski/Documents/data/BirdSet\"\n",
    "HF_DATASETS_CACHE_PATH = f\"{BIG}/hf_datasets\" # Define explicitly for clarity\n",
    "\n",
    "# redirect every HF scratch dir there\n",
    "os.environ[\"HF_HOME\"]           = f\"{BIG}/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = HF_DATASETS_CACHE_PATH\n",
    "os.environ[\"TMPDIR\"]            = f\"{BIG}/tmp\"\n",
    "\n",
    "# ensure dirs exist\n",
    "for d in (os.environ[\"HF_HOME\"], os.environ[\"HF_DATASETS_CACHE\"], os.environ[\"TMPDIR\"]):\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg       = \"XCM\"\n",
    "NUM_PROC  = 8\n",
    "STREAMING = False\n",
    "\n",
    "print(\"env wired up, starting download…\")\n",
    "t0 = time.time()\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"DBD-research-group/BirdSet\",\n",
    "    cfg,\n",
    "    # *** Explicitly set cache_dir ***\n",
    "    cache_dir=HF_DATASETS_CACHE_PATH,\n",
    "    trust_remote_code=True,\n",
    "    num_proc=None if STREAMING else NUM_PROC,\n",
    "    streaming=STREAMING,\n",
    ")\n",
    "\n",
    "if not STREAMING:\n",
    "    for split in ds.keys():\n",
    "        # Adding progress tracking might be helpful here if it's slow\n",
    "        print(f\"Materializing split: {split}\")\n",
    "        # Force materialization by iterating (as you did) or using .map()\n",
    "        # ds[split].map(lambda x: x) # Another way to force processing\n",
    "        for i, _ in enumerate(ds[split]):\n",
    "            if i % 1000 == 0 and i > 0: # Optional progress update\n",
    "                print(f\"  Processed {i} items in {split}...\")\n",
    "            pass\n",
    "\n",
    "print(f\"✓ finished {cfg} in {(time.time()-t0)/60:.1f} min  (all data lives under “{BIG}”)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweetybert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
